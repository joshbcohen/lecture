For the questions regarding the Game of Life, you may want to refer
to the simple implementation included in the "life" subdirectory.
If you run "make glider", you can see a small example of running
the glider pattern for a few generations.

0.  How much time did you spend on this pre-class exercise, and when?

    ANSWER:

    ~1-2 hours, end of semester

1.  What are one or two points that you found least clear in the
    9/15 slide decks (including the narration)?

    ANSWER:

    Slides were pretty clear

2.  In the basic implementation provided, what size board for the Game
    of Life would fit in L3 cache for one of the totient nodes?  Add a
    timer to the code and run on the totient node.  How many cells per
    second can we update for a board that fits in L3 cache?  For a
    board that does not fit?

    ANSWER:

    http://www.cpu-world.com/CPUs/Xeon/Intel-Xeon%20E5-2620.html
    15 MB L3 cache = 15278640 bytes
    1 byte needed for each cell
    15278640 cells
    A maximum 3908*3908 board can fit

    Compiled w/ icc

    Around 1.2 * 10^8 cells/second for a 100*100 board (can fit), 1.7*10^8 for 4000 * 4000 board (can't fit)

3.  Assuming that we want to advance time by several generations,
    suggest a blocking strategy that would improve the operational
    intensity of the basic implementation.  Assume the board is
    not dilute, so that we must still consider every cell.  You may
    want to try your hand at implementing your strategy (though you
    need not spend too much time on it).

    ANSWER:

    Similar strategy to what's presented in slide deck- partition overall grid
    into a number of smaller subgrids, run simulations on each subgrid.

4.  Comment on what would be required to parallelize this code
    according to the domain decomposition strategy outlined in the
    slides.  Do you think you would see good speedups on one of
    the totient nodes?  Why or why not?

    ANSWER:

    - Block in such a way that each processor controls one block (so # of processors equals # of blocks).
    - Cells immediately adjacent to each block boundary need to have their state communicated between the
      processors at those boundaries, in case some state is transferred between processors
    - Can parallelize using MPI
    - Probably would have decent speedups, but it would depend on the number of processors and the size of the board
      For smaller boards, the cost of message passing might make this tuning not worth it


5.  Suppose we want to compute long-range interactions between two
    sets of particles in parallel using the obvious n^2 algorithm in a
    shared-memory setting.  A naive implementation might look like

      struct particle_t {
          double x, y;
          double fx, fy;
      };

      // Update p1 with forces from interaction with p2
      void apply_forces(particle* p1, particle* p2);

      // Assume p is the index of the current processor,
      // part_idx[p] <= i < part_idx[p+1] is the range of
      // particles "owned" by processor p.
      //
      for (int i = part_idx[p]; i < part_idx[p+1]; ++i)
          for (int j = 0; j < npart; ++j)
              apply_forces(particle + i, particle + j);

    Based on what you know about memories and parallel architecture,
    do you see any features of this approach that are likely to lead
    to poor performance?

    ANSWER:

    The approach detailed here doesn't really have any way of dealing with poor
    load balancing or poor locality because it just goes through the
    processors in a round robin fashion and iterates through the particles
    sequentially, so there could be issues with poor locality within the
    particles assigned to a given processor or when calculating
    particle interactions.
